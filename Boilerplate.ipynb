{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d80f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://hackernoon.com/reinforcement-learning-part-2-the-q-learning-algorithm\n",
    "import random\n",
    "\n",
    "def train(n_episodes: int):\n",
    "    \"\"\"\n",
    "    Pseudo-code of a Reinforcement Learning agent training loop\n",
    "    \"\"\"\n",
    "\n",
    "    # python object that wraps all environment logic. Typically you will\n",
    "    # be using OpenAI gym here.\n",
    "    env = load_env()\n",
    "\n",
    "    # python object that wraps all agent policy (or value function)\n",
    "    # parameters, and action generation methods.\n",
    "    agent = get_rl_agent()\n",
    "\n",
    "    for episode in range(0, n_episodes):\n",
    "\n",
    "        # random start of the environmnet\n",
    "        state = env.reset()\n",
    "\n",
    "        # epsilon is parameter that controls the exploitation-exploration trade-off.\n",
    "        # it is good practice to set a decaying value for epsilon\n",
    "        epsilon = get_epsilon(episode)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                # Explore action space\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Exploit learned values (or policy)\n",
    "                action = agent.get_best_action(state)\n",
    "\n",
    "            # environment transitions to next state and maybe rewards the agent.\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # adjust agent parameters. We will see how later in the course.\n",
    "            agent.update_parameters(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced21347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b050154",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/kkm32b053lsfrvg3wp66htdm0000gn/T/ipykernel_15007/609894796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#    episode has ended (i.e. the driver has dropped the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#    passenger at the correct destination)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'env.P[state][action][0]: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# env.P is double dictionary.\n",
    "# - The 1st key represents the state, from 0 to 499\n",
    "# - The 2nd key represens the action taken by the agent,\n",
    "#   from 0 to 5\n",
    "\n",
    "# example\n",
    "state = 123\n",
    "action = 0  # move south\n",
    "\n",
    "# env.P[state][action][0] is a list with 4 elements\n",
    "# (probability, next_state, reward, done)\n",
    "# \n",
    "#  - probability\n",
    "#    It is always 1 in this environment, which means\n",
    "#    there are no external/random factors that determine the\n",
    "#    next_state\n",
    "#    apart from the agent's action a.\n",
    "#\n",
    "#  - next_state: 223 in this case\n",
    "# \n",
    "#  - reward: -1 in this case\n",
    "#\n",
    "#  - done: boolean (True/False) indicates wheter the\n",
    "#    episode has ended (i.e. the driver has dropped the\n",
    "#    passenger at the correct destination)\n",
    "print('env.P[state][action][0]: ', env.P[state][action][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e75cf8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/kkm32b053lsfrvg3wp66htdm0000gn/T/ipykernel_15007/1320512185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Need to call reset() at least once before render() will work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Need to call reset() at least once before render() will work\n",
    "env.reset()\n",
    "\n",
    "env.s = 123\n",
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc25930",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/kkm32b053lsfrvg3wp66htdm0000gn/T/ipykernel_15007/1351726816.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m223\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.s = 223\n",
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27249100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77aec21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a49c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7900a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c7567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa637f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80368b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886268af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4efb892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c127a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107c310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1d0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfa2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf0c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24eb1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cd4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b4dcc1",
   "metadata": {},
   "source": [
    "3. Random agent baseline ðŸ¤–ðŸ·\n",
    "ðŸ‘‰ðŸ½ notebooks/01_random_agent_baseline.ipynb\n",
    "\n",
    "Before you start implementing any complex algorithm, you should always build a baseline model.\n",
    "\n",
    "This advice applies not only to Reinforcement Learning problems but Machine Learning problems in general.\n",
    "\n",
    "It is very tempting to jump straight into the complex/fancy algorithms, but unless you are really experienced, you will fail terribly.\n",
    "\n",
    "Letâ€™s use a random agent ðŸ¤–ðŸ· as a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033770b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    This taxi driver selects actions randomly.\n",
    "    You better not get into this taxi!\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def get_action(self, state) -> int:\n",
    "        \"\"\"\n",
    "        We have `state` as an input to keep\n",
    "        a consistent API for all our agents, but it\n",
    "        is not used.\n",
    "        \n",
    "        i.e. The agent does not consider the state of\n",
    "        the environment when deciding what to do next.\n",
    "        This is why we call it \"random\".\n",
    "        \"\"\"\n",
    "        return self.env.action_space.sample()\n",
    "\n",
    "agent = RandomAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b5755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3a531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002303d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24]) torch.Size([24])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False])\n",
      "tensor([[[1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [0, 0],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_open_hours_mask(DAY_SIZE):\n",
    "    \n",
    "    open_hour_mask = torch.tensor([ True, True, True, True, True, True, True, True, False, False,\n",
    "        False, False, True, True, False, False, False, True, True, True,\n",
    "        True, True, True, True])\n",
    "    \n",
    "    return open_hour_mask\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "DAY_SIZE = 24        ### per hour\n",
    "CALENDAR_SIZE = 4    ### four days\n",
    "NB_AMO = 2           ### one AMO\n",
    "\n",
    "# DAYS (D) x OPEN HOURS (OH) x NB AMO (NA)\n",
    "calendar_model = torch.ones(CALENDAR_SIZE,int(DAY_SIZE), NB_AMO).int()\n",
    "\n",
    "# calendar = torch.ones(DAY_SIZE).int() # D\n",
    "\n",
    "calendar = get_open_hours_mask(DAY_SIZE).int()\n",
    "\n",
    "calendar_mask = torch.zeros(DAY_SIZE).bool() # D \n",
    "calendar_mask[0] = True\n",
    "\n",
    "calendar_mask_model = calendar.unsqueeze(1).expand(calendar_model.size()) # D x OH x NA\n",
    "\n",
    "print(calendar.size(), calendar_mask.size())\n",
    "# print(calendar[0,:,1], calendar_mask[2,:,1])\n",
    "\n",
    "print(calendar)\n",
    "print(calendar_mask)\n",
    "print(calendar_mask_model)\n",
    "\n",
    "\n",
    "# masked_cal = calendar & calendar_mask # D \n",
    "\n",
    "# print(masked_cal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7f3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cee1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    Blocks of rendez-vous are put on the agenda randomly\n",
    "    dimension: DAY, HOUR, AMO\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def get_action(self, state) -> int:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3dbc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ce462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d8ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3db7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a00cea6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/kkm32b053lsfrvg3wp66htdm0000gn/T/ipykernel_15007/275515262.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set initial state of the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# set initial state of the environment\n",
    "env.reset()\n",
    "state = 123\n",
    "env.s = state\n",
    "\n",
    "epochs = 0\n",
    "penalties = 0  # wrong pick up or dropp off\n",
    "reward = 0\n",
    "\n",
    "# store frames to latter plot them\n",
    "frames = []\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = agent.get_action(state)\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dc17e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/kkm32b053lsfrvg3wp66htdm0000gn/T/ipykernel_15007/4002480430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# reset environment to a random state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_episodes = 100\n",
    "\n",
    "# For plotting metrics\n",
    "timesteps_per_episode = []\n",
    "penalties_per_episode = []\n",
    "\n",
    "for i in tqdm(range(0, n_episodes)):\n",
    "    \n",
    "    # reset environment to a random state\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = agent.get_action(state)       \n",
    "        next_state, reward, done, info = env.step(action) \n",
    "               \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "    \n",
    "    timesteps_per_episode.append(epochs)\n",
    "    penalties_per_episode.append(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a01a63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/kkm32b053lsfrvg3wp66htdm0000gn/T/ipykernel_15007/3084782025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Avg steps to complete ride: {np.array(timesteps_per_episode).mean()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Avg penalties to complete ride: {np.array(penalties_per_episode).mean()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Avg steps to complete ride: {np.array(timesteps_per_episode).mean()}')\n",
    "print(f'Avg penalties to complete ride: {np.array(penalties_per_episode).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f62c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1268e668",
   "metadata": {},
   "source": [
    "# Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398f6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QAgent:\n",
    "\n",
    "    def __init__(self, env, alpha, gamma):\n",
    "        self.env = env\n",
    "\n",
    "        # table with q-values: n_states * n_actions\n",
    "        self.q_table = np.zeros([env.observation_space.n,\n",
    "                                 env.action_space.n])\n",
    "\n",
    "        # hyper-parameters\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\"\"\"\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_parameters(self, state, action, reward, next_state):\n",
    "        \"\"\"\"\"\"\n",
    "        # Q-learning formula\n",
    "        old_value = self.q_table[state, action]\n",
    "        next_max = np.max(self.q_table[next_state])\n",
    "        new_value = \\\n",
    "            old_value + \\\n",
    "            self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "\n",
    "        # update the q_table\n",
    "        self.q_table[state, action] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43142dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/kkm32b053lsfrvg3wp66htdm0000gn/T/ipykernel_15007/680295490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# exploration vs exploitation prob\n",
    "epsilon = 0.1\n",
    "\n",
    "n_episodes = 10000\n",
    "\n",
    "# For plotting metrics\n",
    "timesteps_per_episode = []\n",
    "penalties_per_episode = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, n_episodes)):\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Explore action space\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit learned values\n",
    "            action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        agent.update_parameters(state, action, reward, next_state)\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    timesteps_per_episode.append(epochs)\n",
    "    penalties_per_episode.append(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe69155",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/kkm32b053lsfrvg3wp66htdm0000gn/T/ipykernel_15007/2877383420.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timesteps to complete ride\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Timesteps to complete ride\")    \n",
    "pd.Series(timesteps_per_episode).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Penalties per ride\")    \n",
    "pd.Series(penalties_per_episode).plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bf0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58756a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
